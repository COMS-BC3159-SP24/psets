{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PS2: Matrix Inverse (8 Points)\n",
        "\n",
        "### Problem Statement\n",
        "In this assignment you are going to wite a kernel to compute a matrix inverse which we can then use to solve a linear system. We have provided some starter code for you, and your job is to fill in the missing code as specified by the `#TODO#` blocks in the code. You can either just work in the ipynb OR you can work locally with the various files in this folder. If you don't have a GPU on your machine you will need to upload the ipynb to [Google Colaboratory](https://colab.research.google.com/) and work there.\n",
        "\n",
        "### Functions You'll Need To Implement\n",
        "All functions you need to implement are in `util.h` and that is the only file you need to submit to courseworks! Please do not change function definitions. Also be careful with your syncs!\n",
        "\n",
        "**`matrix_inverse_inner` (5 points):** this device function will do all of the heavy lifting and actually implement the matrix inverse function. It takes in the input matrix and outputs the inverse. You should assume these pointers are already in shared memory. There is also an additional input of additional shared temporary memory which you may or may not need to use (and you will get to specify how big it is in later functions). You can implement this in any way, but/and the simplest way to do this is through the use of the Gauss Jordan elimination method. You can see some graphical examples of \"elementary row operations\" [at this link](https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html). But/and in short you can simply walk through the rows in order (aka do not swap any rows and just do them in order) and for each row divide it to ensure that it is leading with a 1 and then subtract down along the rest of the rows to ensure they are all leading with a 0. If you repeat this and move down and to the right through the matrix you will end up with the identity in place of the original matrix and the inverse to the right! You can assume that the input matrix does not have a zero in the top left corner and is of a relatively small size (aka less than 30x30) and that it is a square matrix. The function input `matrix_dim` is the number of rows (and since the matrix is square also the number of columns) of the input matrix.\n",
        "\n",
        "Note: there are a number of ways to actually implement it. Do whatever you think is simplest and easiest first. Then see if you can optimize it!\n",
        "\n",
        "**`matrix_inverse_kernel` (2 points):** this kernel will call the `_inner` function and will and should move the device matrix into shared memory, make sure the computation occurs in shared memory, and return the value to device memory. You will note that we specified dynamic shared memory. Please use that and make sure to allocate enough of it in the host function to support however much your device function needs! As with `_inner`, the function input `matrix_dim` is the number of rows (and since the matrix is square also the number of columns) of the input matrix.\n",
        "\n",
        "**`matrix_inverse` (1 point):** the host function will call the `_kernel` and will move the input host matrix into device memory, launch the kernel (with dynamic shared memory), and return the solution to host memory. As with `_inner` and `_kernel`, the function input `matrix_dim` is the number of rows (and since the matrix is square also the number of columns) of the input matrix.\n",
        "\n",
        "### Example Output\n",
        "Please see below for inline example output from Python.\n",
        "\n",
        "### Submission\n",
        "Once you are done, download and submit (or just submit if you are working locally) your `ipynb` or `util.h` file to **Courseworks**! Sadly Gradescope does not support GPU autograders at this time.\n",
        "\n",
        "### Notes and Hints\n",
        "+ **DO NOT CHANGE FUNCTION DEFINITIONS** or you will break our grading scripts\n",
        "+ See the syllabus for our course collaboration policy (long story short you are welcome to collaborate at a high level but please do not copy each others code).\n",
        "+ Please reach out on Slack with any and all questions!"
      ],
      "metadata": {
        "id": "wv-U27eXCm90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5rjK8n-CDpz"
      },
      "outputs": [],
      "source": [
        "# make sure CUDA is installed\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure you have a GPU runtime (if this fails go to runtime -> change runtime type)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "1E43c7ZPH71B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA in Jupyter helpers\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter\n",
        "# to learn about how to do more fancy things with CUDA using this API see:\n",
        "# https://nvcc4jupyter.readthedocs.io/en/latest/index.html"
      ],
      "metadata": {
        "id": "LhFHYH9zH8JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Code Goes Here:"
      ],
      "metadata": {
        "id": "wTTdEYt1aCk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -n util.h -g default\n",
        "\n",
        "// the GPU device function\n",
        "//\n",
        "// Hints: actually solve the matrix inverse!\n",
        "//\n",
        "__device__\n",
        "void matrix_inverse_inner(float *s_input, float *s_output, float *s_temp, const int matrix_dim){\n",
        "  // first set up the matrix with identity next to it\n",
        "  // #TODO\n",
        "\n",
        "\n",
        "  // Next we are going to guassian elimination walking down the matrix (assuming no leading 0s).\n",
        "  // We therefore use the columns in order as the pivot column for each pivot we need to rescale\n",
        "  // that row so that the pivot value is 1 THEN for all other row values we need to add a multiple\n",
        "  // of the NEW pivot row value such that we transorm the other row pivot column value to 0.\n",
        "  // See https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html\n",
        "  //\n",
        "  // Note if you would prefer to use another method that is fine but/and this is the method\n",
        "  // we have a solution for and are prepared to help you with!\n",
        "  for (unsigned pivRC = 0; pivRC < matrix_dim; pivRC++){\n",
        "      //\n",
        "      // #TODO\n",
        "      //\n",
        "  }\n",
        "\n",
        "  // Make sure to write the result to the output\n",
        "  // # TODO\n",
        "}\n",
        "\n",
        "// the GPU kernel\n",
        "//\n",
        "// Hints: set up shared memory, run the _inner, clean up shared memory\n",
        "//\n",
        "__global__\n",
        "void matrix_inverse_kernel(float *d_input, float *d_output, const int matrix_dim){\n",
        "  // get shared pointers\n",
        "  extern __shared__ float s_dynShared[];\n",
        "  // #TODO\n",
        "\n",
        "  // copy the data into shared memory\n",
        "  // #TODO\n",
        "\n",
        "  // run the code\n",
        "  // #TODO\n",
        "\n",
        "  // copy the memory back out\n",
        "  // #TODO\n",
        "}\n",
        "\n",
        "// the host function\n",
        "//\n",
        "// Hints: set up GPU memory, run the kernel, clean up GPU memory\n",
        "//\n",
        "__host__\n",
        "void matrix_inverse(float *h_input, float *h_output, const int matrix_dim){\n",
        "  // transfer memory to the device\n",
        "  // #TODO\n",
        "\n",
        "  // run the kernel\n",
        "  // #TODO\n",
        "\n",
        "  // transfer data back to the host\n",
        "  // #TODO\n",
        "}"
      ],
      "metadata": {
        "id": "_F0I9vozH-KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to run and test your code follows:"
      ],
      "metadata": {
        "id": "VA2qDKZ8aFZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -n run.cu -g default\n",
        "#include <stdio.h>\n",
        "#include \"util.h\"\n",
        "\n",
        "__host__\n",
        "void printMat(float *mat, const int matrix_dim){\n",
        "    // loop through row by row and print\n",
        "    for (int r = 0; r < matrix_dim; r++){\n",
        "        for (int c = 0; c < matrix_dim; c++){\n",
        "            printf(\"%f \",mat[r + c*matrix_dim]);\n",
        "        }\n",
        "        // Newline for new row\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    // Newline for end of print\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "__host__\n",
        "int runTest(float *h_input, float *h_output, const int matrix_dim){\n",
        "  // print the input matrix\n",
        "  printMat(h_input,matrix_dim);\n",
        "\n",
        "  // run the main function\n",
        "  matrix_inverse(h_input,h_output,matrix_dim);\n",
        "\n",
        "  // print the final result\n",
        "  printMat(h_output,matrix_dim);\n",
        "}\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "\n",
        "  // initialize the first test matrix\n",
        "  const int matrix_dim = 5;\n",
        "  const int matrix_dim_sq = matrix_dim*matrix_dim;\n",
        "  float *h_input = (float *)malloc(matrix_dim_sq*sizeof(float));\n",
        "  float *h_output = (float *)malloc(matrix_dim_sq*sizeof(float));\n",
        "  for (int c = 0; c < matrix_dim; c++){\n",
        "      for (int r = 0; r < matrix_dim; r++){\n",
        "          h_input[r + c*matrix_dim] = (r == c) ? 2 : 0;\n",
        "      }\n",
        "  }\n",
        "  // run the test\n",
        "  runTest(h_input,h_output,matrix_dim);\n",
        "\n",
        "  // now do the second test\n",
        "  for (int c = 0; c < matrix_dim; c++){\n",
        "      for (int r = 0; r < matrix_dim; r++){\n",
        "          h_input[r + c*matrix_dim] = r >= c ? 7 : 0;\n",
        "      }\n",
        "  }\n",
        "  runTest(h_input,h_output,matrix_dim);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "QLEV1yHLIG01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cuda_group_run -g default"
      ],
      "metadata": {
        "id": "Vc-8tCkjTzDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of your code above should match the Python code below!"
      ],
      "metadata": {
        "id": "yMMDaxn3aIWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "test1 = 2*np.eye(5)\n",
        "print(test1)\n",
        "print(np.linalg.inv(test1))\n",
        "test2 = 7*np.tri(5)\n",
        "print(test2)\n",
        "print(np.linalg.inv(test2))"
      ],
      "metadata": {
        "id": "FICgnecKT3Wc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}